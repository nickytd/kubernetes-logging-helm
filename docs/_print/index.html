<!doctype html><html lang=en class=no-js>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.91.2">
<link rel=canonical type=text/html href=https://nickytd.github.io/kubernetes-logging-helm/docs/>
<link rel=alternate type=application/rss+xml href=https://nickytd.github.io/kubernetes-logging-helm/docs/index.xml>
<meta name=robots content="noindex, nofollow">
<link rel="shortcut icon" href=/kubernetes-logging-helm/favicons/favicon.ico>
<link rel=apple-touch-icon href=/kubernetes-logging-helm/favicons/apple-touch-icon-180x180.png sizes=180x180>
<link rel=icon type=image/png href=/kubernetes-logging-helm/favicons/favicon-16x16.png sizes=16x16>
<link rel=icon type=image/png href=/kubernetes-logging-helm/favicons/favicon-32x32.png sizes=32x32>
<link rel=icon type=image/png href=/kubernetes-logging-helm/favicons/android-36x36.png sizes=36x36>
<link rel=icon type=image/png href=/kubernetes-logging-helm/favicons/android-48x48.png sizes=48x48>
<link rel=icon type=image/png href=/kubernetes-logging-helm/favicons/android-72x72.png sizes=72x72>
<link rel=icon type=image/png href=/kubernetes-logging-helm/favicons/android-96x96.png sizes=96x96>
<link rel=icon type=image/png href=/kubernetes-logging-helm/favicons/android-144x144.png sizes=144x144>
<link rel=icon type=image/png href=/kubernetes-logging-helm/favicons/android-192x192.png sizes=192x192>
<title>Helm chart overview | OFD Logging helm chart</title>
<meta name=description content="
This helm chart deploys a scalable containerized logging stack with the main purpose of enabling log observability for kubernetes applications. The …">
<meta property="og:title" content="Helm chart overview">
<meta property="og:description" content="Helm chart for logging stack based on OFD for Kubernetes">
<meta property="og:type" content="website">
<meta property="og:url" content="https://nickytd.github.io/kubernetes-logging-helm/docs/"><meta property="og:site_name" content="OFD Logging helm chart">
<meta itemprop=name content="Helm chart overview">
<meta itemprop=description content="Helm chart for logging stack based on OFD for Kubernetes"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Helm chart overview">
<meta name=twitter:description content="Helm chart for logging stack based on OFD for Kubernetes">
<link rel=preload href=/kubernetes-logging-helm/scss/main.min.209ee91954d0f2f182770e6c71011a5580360636bcf2a9bcf7f1c3da478784ff.css as=style>
<link href=/kubernetes-logging-helm/scss/main.min.209ee91954d0f2f182770e6c71011a5580360636bcf2a9bcf7f1c3da478784ff.css rel=stylesheet integrity>
<script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
<a class=navbar-brand href=/kubernetes-logging-helm/>
<span class=navbar-logo></span><span class=font-weight-bold>OFD Logging helm chart</span>
</a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/kubernetes-logging-helm/><span></span></a>
</li>
</ul>
</div>
<div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/kubernetes-logging-helm/offline-search-index.c9dcebfe19f8f06c6fb0c3ede8089746.json data-offline-search-base-href=/ data-offline-search-max-results=10>
</div>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.
</p><p>
<a href=/kubernetes-logging-helm/docs/>Return to the regular view of this page</a>.
</p>
</div>
<h1 class=title>Helm chart overview</h1>
<ul>
<li>1: <a href=#pg-da1dd8fb56a8c18860485b01c003d92f>Deployment Layouts</a></li>
<ul>
</ul>
<li>2: <a href=#pg-cc384b22fa40feda6068c9b9bef374a6>Logging Stack Components</a></li>
<ul>
<li>2.1: <a href=#pg-8302c08d11419415527fa4a32b16a09b>FluentBit</a></li>
<ul>
</ul>
<li>2.2: <a href=#pg-c8be4e2232f77cef77deb84e15e22eda>OpenSearch</a></li>
<ul>
<li>2.2.1: <a href=#pg-63db1ee76d05f13a8e7f9edb2a3df1f3>Index Management</a></li>
<ul>
</ul>
<li>2.2.2: <a href=#pg-2fc083bd0278a17dc1d35efc939c811e>Possible problems</a></li>
<ul>
<li>2.2.2.1: <a href=#pg-e4eb219fa55e4ace518681faad903db2>Missing data node</a></li>
<ul>
</ul>
</ul>
</ul>
<li>2.3: <a href=#pg-64c562ddfdf1515623da37aa02133a86>OpenSearch-Dashboards</a></li>
<ul>
<li>2.3.1: <a href=#pg-1c9f0942d4ab7bc96d6a676e1e85e588>Opensearch-Dashboards Authentication & Authorization</a></li>
<ul>
</ul>
<li>2.3.2: <a href=#pg-b96842a864c9bbea35a261097c779f33>OpenSearch-Dashboards-Observability</a></li>
<ul>
</ul>
<li>2.3.3: <a href=#pg-7fcc1ea9f1032bbae82f10dfb538a0cf>Opensearch-Dashboards Visualizations</a></li>
<ul>
</ul>
</ul>
<li>2.4: <a href=#pg-3a8656ebec6b78171def1a3542380037>Kafka</a></li>
<ul>
<li>2.4.1: <a href=#pg-febfba28b8eb6627e21e996fb64a46f8>Howtos</a></li>
<ul>
<li>2.4.1.1: <a href=#pg-d2aa4f1d32e931ed2096133d7d8c74c8>How to generating cluster ID</a></li>
<ul>
</ul>
</ul>
</ul>
</ul>
<li>3: <a href=#pg-5cb0fd650727bc1f9017ddcace2f3c54>Upgrade Notes</a></li>
<ul>
</ul>
<li>4: <a href=#pg-4553fe28b20f652961f013e3fc49ad6a>Helm chart values</a></li>
<ul>
</ul>
<li>5: <a href=#pg-4588103fb9a692533044ee756a5f863f>Frequently asked questions</a></li>
<ul>
</ul>
</ul>
<div class=content>
<p><img src=../opensearch-k8s.png alt="Basic logo"></p>
<p>This helm chart deploys a scalable containerized logging stack with the main purpose of enabling log observability for kubernetes applications. The design supports both local development use cases such as minikube deployments up to a scaled production scenarios. The log shippers are <a href=https://fluentbit.io/>FluentBits</a> deployed on each kubernetes node mounting the host filesystem. The latter scenarios leverage <a href=https://kafka.apache.org/>Kafka</a> message broker, completely decoupling in this way, the log generation and log indexing functions.</p>
<p>The helm chart supports <a href=https://opensearch.org/>OpenSearch</a> in various configurations starting from a single node setup usable for local development, to a scaled multi nodes OpenSearch deployment suitable for production environment. In the latter case there are 3 types of nodes (coordination, data and master) where each of those can be both horizontally and vertically scaled depending on the load and shards replication demands.</p>
<p>Finally this helm chart provides index templates management in OpenSearch and index pattern management in <a href=https://opensearch.org/docs/latest/dashboards/index/>OpenSearchDashboards</a>. An initial predefined set of dashboards is also provided for illustration purposes.</p>
<p><img src=../k8s-logging-stack.jpg alt=Schema></p>
<h2 id=adding-the-helm-chart-repository>Adding the helm chart repository:</h2>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>helm repo add logging https://nickytd.github.io/kubernetes-logging-helm
helm repo update
</code></pre></div>
<div class="alert alert-primary" role=alert>
<h4 class=alert-heading>Note</h4>
Any authenticated user should have <strong>read access</strong> to the helm repository.
</div>
<h2 id=prepare-a-release-configuration>Prepare a release configuration</h2>
<p>The recommended approach is the get the default <a href=https://github.com/nickytd/kubernetes-logging-helm/blob/main/chart/values.yaml>helm chart values</a> and adjust accordingly.
At minimum the ingress annotations for the OpenSearch rest endpoint and OpenSearchDashboards UI app have to be adjusted. Here is an <a href=https://github.com/nickytd/kubernetes-logging-helm/blob/main/examples/single-node-setup.yaml>example</a> for a minimal single OpenSearch node setup.</p>
<h2 id=install-a-release>Install a release</h2>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>helm install ofd logging/kubernetes-logging
</code></pre></div>
</div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-da1dd8fb56a8c18860485b01c003d92f>1 - Deployment Layouts</h1>
<div class=lead>Overview on available installation layouts</div>
<p>The kubernetes logging helm chart supports a number of deployment layouts of OpenSearch and other components depending on the concrete purpose and size of the cluster.</p>
<h2 id=single-node-opensearch>Single node OpenSearch</h2>
<p><img src=./kubernetes-logging-single-node.png alt=single-node></p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#ff79c6>opensearch</span>:
  <span style=color:#ff79c6>single_node</span>: <span style=color:#ff79c6>true</span>

<span style=color:#ff79c6>fluentbit</span>:
  <span style=color:#ff79c6>containersLogsHostPath</span>: /var/log/pods
  <span style=color:#ff79c6>journalsLogsHostPath</span>: /var/log
  <span style=color:#ff79c6>containersRuntime</span>: docker

<span style=color:#ff79c6>kafka</span>:
  <span style=color:#ff79c6>false</span>:
</code></pre></div><p>This layout is the simplest possible requiring the least compute and memory resources. It comprises of the log shippers, a single OpenSearch node and a single OpenSearchDashabord UI. The log shippers are FluentBits deployed on each kubernetes node mounting the host filesystem. Because the locations of the containers logs or the host journals can vary, those locations have to be adapted accordingly in the FluentBit configuration. The logs are directly send to the Opensearch node for indexing without the need of a message broker in between.</p>
<blockquote>
<p><strong>Recommendation:</strong> Although the single node can be scaled by simply increasing the replicas in the &ldquo;data&rdquo; configuration, this setup is most suitable for development environments like minikube or kind clusters.</p>
</blockquote>
<h2 id=multi-node-opensearch>Multi node OpenSearch</h2>
<p><img src=./kubernetes-logging-multi-node.png alt=multi-node></p>
<p>OpenSearch supports dedicated <a href=https://opensearch.org/docs/latest/opensearch/cluster/>node types</a> based on specific functions in the cluster. A <em>coordination node</em>, <em>data node</em> and <em>cluster manager node</em> forming an OpenSearch cluster can be deployed when <code>single_node</code> option is disabled.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#ff79c6>opensearch</span>:
  <span style=color:#ff79c6>single_node</span>: <span style=color:#ff79c6>false</span>
</code></pre></div><h2 id=scaled-multi-node-opensearch-in-production>Scaled multi node OpenSearch in production</h2>
<p><img src=./kubernetes-logging-multi-node-production.png alt=multi-node-production></p>
<p>When the setup is deployed in a production environment both aspects for reliably and throughout of the logs streams are addressed by the helm chart with the introduction of a message broker. A running message broker (Kafka) effectively accumulates spikes of logs volumes or downtimes of the backend OpenSearch cluster.</p>
<div class="alert alert-warning" role=alert>
<h4 class=alert-heading>Note:</h4>
Kafka and Logstash needs to be enabled as well!.
</div>
<blockquote>
<p>Delivery chain is: <code>Kafka</code> -> <code>Logstash</code> -> <code>OpenSearch</code></p>
</blockquote>
<p>Even more importantly each component can be scaled horizontally insuring better reliability.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#ff79c6>opensearch</span>:
  <span style=color:#ff79c6>single_node</span>: <span style=color:#ff79c6>false</span>

<span style=color:#ff79c6>data</span>:
  <span style=color:#ff79c6>replicas</span>: <span style=color:#bd93f9>3</span>

<span style=color:#ff79c6>clusterManager</span>:
  <span style=color:#ff79c6>replicas</span>: <span style=color:#bd93f9>3</span>

<span style=color:#ff79c6>client</span>:
  <span style=color:#ff79c6>replicas</span>: <span style=color:#bd93f9>3</span>

<span style=color:#ff79c6>kafka</span>:
  <span style=color:#ff79c6>enabled</span>: <span style=color:#ff79c6>true</span>
  <span style=color:#ff79c6>replicas</span>: <span style=color:#bd93f9>3</span>

<span style=color:#ff79c6>logstash</span>:
  <span style=color:#ff79c6>enabled</span>: <span style=color:#ff79c6>true</span>
  <span style=color:#ff79c6>replicas</span>: <span style=color:#bd93f9>3</span>

<span style=color:#6272a4>#and so on</span>
</code></pre></div><p>Additionally each type of workload scheduling strategy can be further optimized by defining node and pods (anti)affinity rules.</p>
<p>For example for stateful sets like Kafka&rsquo;s or data nodes following affinity strategy guarantees that pods will be scheduled on different kubernetes nodes.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#ff79c6>affinity</span>:
  <span style=color:#ff79c6>podAntiAffinity</span>:
    <span style=color:#ff79c6>preferredDuringSchedulingIgnoredDuringExecution</span>:
    - <span style=color:#ff79c6>weight</span>: <span style=color:#bd93f9>100</span>
      <span style=color:#ff79c6>podAffinityTerm</span>:
        <span style=color:#ff79c6>labelSelector</span>:
          <span style=color:#ff79c6>matchExpressions</span>:
          - <span style=color:#ff79c6>key</span>: type
            <span style=color:#ff79c6>operator</span>: In
            <span style=color:#ff79c6>values</span>:
            <span style=color:#6272a4># with the corresponding label</span>
            - kafka
        <span style=color:#ff79c6>topologyKey</span>: kubernetes.io/hostname
</code></pre></div><p>Or in the case of a deployments like the OpenSearch coordination nodes or <a href=https://opensearch.org/docs/latest/clients/logstash/index/>Logstashes</a> a spread of pods over nodes can be achieved with:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#ff79c6>topologySpreadConstraints</span>:
  - <span style=color:#ff79c6>maxSkew</span>: <span style=color:#bd93f9>1</span>
    <span style=color:#ff79c6>topologyKey</span>: kubernetes.io/hostname
    <span style=color:#ff79c6>whenUnsatisfiable</span>: ScheduleAnyway
    <span style=color:#ff79c6>labelSelector</span>:
      <span style=color:#ff79c6>matchLabels</span>:
        <span style=color:#6272a4># with the corresponding label</span>
        <span style=color:#ff79c6>type</span>: client
</code></pre></div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-cc384b22fa40feda6068c9b9bef374a6>2 - Logging Stack Components</h1>
</div>
<div class=td-content>
<h1 id=pg-8302c08d11419415527fa4a32b16a09b>2.1 - FluentBit</h1>
<div class=lead>Configuration settings for FluentBit log shipper</div>
<p><a href=https://fluentbit.io/>FluentBit</a> is installed as daemon set on each of the k8s nodes by the helm chart. It follows FluentBit <a href=https://docs.fluentbit.io/manual/concepts/data-pipeline>data pipeline</a> setup designed for kubernetes environments.</p>
<p>The helm chart itself supports different deployment layouts depending on whether a simple or standard model is required. The standard model is recommended in production where various components runs in HA mode. In this case the FluentBit instances send the collected logs to kafka brokers. The kafka brokers are used for buffering and greatly increase the overall reliability and stability of the entire stack.</p>
<p><img src=./fbt_standard_deployment_layout.png alt="standard layout"></p>
<p>In the simple case the FluentBit instances communicate directly with OpenSearch nodes.
<img src=./fbt_simple_deployment_layout.png alt="simple layout"></p>
<p>In both cases there is a set of FluentBit configurations which is responsible for proper logs collection from the containers and enriching those with the respective kubernetes metadata such as namespace of the origin workload, its labels and so on. The metadata is later used in indexing, searchers and visualisations scenarios. This shared configuration is shown on the diagrams here as &ldquo;kubernetes data pipeline&rdquo;.</p>
<p>The &ldquo;kubernetes data pipeline&rdquo; uses standard &ldquo;Tail&rdquo; input plugin to read the logs from the mounted node filesystem, &ldquo;Kube-Tag&rdquo; parser plugin to generate FluentBit tag of the events.
Followed by &ldquo;Kubernetes&rdquo; filter used to add the kubernetes metadata to the events followed by the end by a &ldquo;de_dot&rdquo; filter used to replace dots &ldquo;.&rdquo; with undescores &ldquo;_&rdquo; in event names.</p>
<p>The &ldquo;kubernetes data pipeline&rdquo; is the foundation of any application specific configurations. For example nginx ingress controller produces unstructured access logs. To parse those logs and transform the lines into structured json formatted messages we shall enrich the pipeline with corresponding filters and parsers.</p>
<p><img src=./fbt_nginx_layout.png alt=nginx_access_logs></p>
<p>The nginx access logs parsing example is located at <a href=https://github.com/nickytd/kubernetes-logging-helm/tree/main/chart/fluent-bit-configs>fluentbit-configs</a> folder. Any additional application specific configs needs to be saved in the same location following filenames the naming conventions. Aka filters needs to have &ldquo;filter&rdquo; predix, &ldquo;parsers&rdquo; for parsers and so on.</p>
<p>In the nginx access log example the rewrite_tag filter is used to tag messages originating from containers and which share the <code>app_kubernetes_io/name: ingress-nginx</code> label.</p>
<pre tabindex=0><code>[FILTER]
    Name          rewrite_tag
    Match         kube.*
    Rule          $kubernetes['labels']['app_kubernetes_io/name'] &quot;^(ingress-nginx)$&quot; nginx false
[FILTER]
    Name          parser
    Match         nginx
    Key_Name      log
    Parser        k8s-nginx-ingress
    Reserve_Data  True
</code></pre><p>The messages are tagged and re-emitted in the FluentBit data pipeline. Later matched by the nginx parser which uses regex to construct a json formatted structured message</p>
<pre tabindex=0><code>[PARSER]
    Name         k8s-nginx-ingress
    Format       regex
    Regex        ^(?&lt;host&gt;[^ ]*) - (?&lt;user&gt;[^ ]*) \[(?&lt;time&gt;[^\]]*)\] &quot;(?&lt;method&gt;\S+)(?: +(?&lt;path&gt;[^\&quot;]*?)(?: +\S*)?)?&quot; (?&lt;code&gt;[^ ]*) (?&lt;size&gt;[^ ]*) &quot;(?&lt;referrer&gt;[^\&quot;]*)&quot; &quot;(?&lt;agent&gt;[^\&quot;]*)&quot; (?&lt;request_length&gt;[^ ]*) (?&lt;request_time&gt;[^ ]*) \[(?&lt;proxy_upstream_name&gt;[^ ]*)\] (\[(?&lt;proxy_alternative_upstream_name&gt;[^ ]*)\] )?(?&lt;upstream_addr&gt;[^ ]*) (?&lt;upstream_response_length&gt;[^ ]*) (?&lt;upstream_response_time&gt;[^ ]*) (?&lt;upstream_status&gt;[^ ]*) (?&lt;reg_id&gt;[^ ]*).*$
    Time_Key     time
    Time_Format  %d/%b/%Y:%H:%M:%S %z
</code></pre><p>Additional parsers are supported such as multiline parses allowing to reconstruct java stacktraces into a single message.
Here is an example of such configuration.<br><br>
<code>filter-zookeeper.conf</code>:</p>
<pre tabindex=0><code>    [FILTER]
        Name                  rewrite_tag
        Match                 kube.*.logging.*.*
        Rule                  $kubernetes['labels']['type'] &quot;^(zk)$&quot; zookeeper false
        Emitter_Storage.type  filesystem
    [FILTER]
        Name                  multiline
        Match                 zookeeper
        multiline.parser      zookeeper_multiline
</code></pre><p><code>parser-zookeeper.conf</code></p>
<pre tabindex=0><code>   [MULTILINE_PARSER]
       name            zookeeper_multiline
       type            regex
       flush_timeout   1000
       key_content     log
       # Regex rules for multiline parsing
       # ---------------------------------
       #  - first state always has the name: start_state
       #  - every field in the rule must be inside double quotes
       #
       # rules |  state name  | regex pattern                        | next state name
       # ------|--------------|--------------------------------------|----------------
       rule     &quot;start_state&quot;  &quot;/^(?&lt;exception&gt;[^ ]+:)(?&lt;rest&gt;.*)$/&quot;  &quot;cont&quot;
       rule     &quot;cont&quot;         &quot;/\s+at\s.*/&quot;                          &quot;cont&quot;
</code></pre><blockquote>
<p><strong>Hint:</strong> For high volume logs producers consider adding: <code>Emmiter_Storage.type filesystem</code> property. It allows additional buffering during re-emitting of the events, details see <a href=https://docs.fluentbit.io/manual/pipeline/filters/rewrite-tag>FluentBit rewrite-tag</a>.</p>
</blockquote>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c8be4e2232f77cef77deb84e15e22eda>2.2 - OpenSearch</h1>
<div class=lead>Configuration settings for OpenSearch</div>
<p>Kuberenetes logging helm chart supports multiple deployment layouts of OpenSearch, which both satisfy local development needs where minimum use of resources is required or production layout with additional Kafka brokers and HA setup of the various components.</p>
<p>By default the helm chart configures two indices with corresponding index templates. One index is <code>containers-{YYYY.MM.dd}</code> indexing by default all workloads logs and <code>systemd-{YYYY.MM.dd}</code> for storing journal system logs for &ldquo;kubelet&rdquo; or &ldquo;containerd&rdquo; services running on the respective cluster nodes.
Both indices are created according <a href=https://opensearch.org/docs/latest/opensearch/index-templates/>index templates</a> allowing later on dedicated visualizations in OpenSearch Dahboards UI.</p>
<p>&ldquo;Containers&rdquo; index template uses <a href=https://opensearch.org/docs/latest/opensearch/index-templates/#composable-index-templates>composable pattern</a> and leverages a predefined component template named &ldquo;kubernetes-metadata&rdquo;.</p>
<pre tabindex=0><code>containers  [containers-*]  0  [kubernetes-metadata]
systemd     [systemd-*]     0  []
</code></pre><p>The latter uses kubernetes metadata attached by the FluentBit log shippers to unify its structure among workloads. It shall be also used by any container specific index with the purpose of sharing the same kubernetes fields mappings.</p>
<p>The helm chart deploys all templates extensions found in <a href=https://github.com/nickytd/kubernetes-logging-helm/tree/main/chart/index-templates>index-templates</a> folder.
An example of such index template is nginx, which inherits the mappings in the &ldquo;kubernetes-metadata&rdquo; component templates and adds access logs fields mappings.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
   <span style=color:#ff79c6>&#34;index_patterns&#34;</span>:[
      <span style=color:#f1fa8c>&#34;nginx-*&#34;</span>
   ],
   <span style=color:#ff79c6>&#34;composed_of&#34;</span>:[
      <span style=color:#f1fa8c>&#34;kubernetes-metadata&#34;</span>
   ],
   <span style=color:#ff79c6>&#34;template&#34;</span>:{
      <span style=color:#ff79c6>&#34;settings&#34;</span>:{
         <span style=color:#ff79c6>&#34;index&#34;</span>:{
            <span style=color:#ff79c6>&#34;codec&#34;</span>:<span style=color:#f1fa8c>&#34;best_compression&#34;</span>,
            <span style=color:#ff79c6>&#34;mapping&#34;</span>:{
               <span style=color:#ff79c6>&#34;total_fields&#34;</span>:{
                  <span style=color:#ff79c6>&#34;limit&#34;</span>:<span style=color:#bd93f9>1000</span>
               }
            },
            <span style=color:#ff79c6>&#34;number_of_shards&#34;</span>:<span style=color:#f1fa8c>&#34;{{ (.Values.data.replicas | int) }}&#34;</span>,
            <span style=color:#ff79c6>&#34;number_of_replicas&#34;</span>:<span style=color:#f1fa8c>&#34;{{ (sub (.Values.data.replicas | int) 1) }}&#34;</span>,
            <span style=color:#ff79c6>&#34;refresh_interval&#34;</span>:<span style=color:#f1fa8c>&#34;5s&#34;</span>
         }
      },
      <span style=color:#ff79c6>&#34;mappings&#34;</span>:{
         <span style=color:#ff79c6>&#34;_source&#34;</span>:{
            <span style=color:#ff79c6>&#34;enabled&#34;</span>:<span style=color:#ff79c6>true</span>
         },
         <span style=color:#ff79c6>&#34;properties&#34;</span>:{
            <span style=color:#ff79c6>&#34;log&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;text&#34;</span>
            },
            <span style=color:#ff79c6>&#34;agent&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            },
            <span style=color:#ff79c6>&#34;code&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            },
            <span style=color:#ff79c6>&#34;host&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            },
            <span style=color:#ff79c6>&#34;method&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            },
            <span style=color:#ff79c6>&#34;path&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            },
            <span style=color:#ff79c6>&#34;proxy_upstream_name&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            },
            <span style=color:#ff79c6>&#34;referrer&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            },
            <span style=color:#ff79c6>&#34;reg_id&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            },
            <span style=color:#ff79c6>&#34;request_length&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;long&#34;</span>
            },
            <span style=color:#ff79c6>&#34;request_time&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;double&#34;</span>
            },
            <span style=color:#ff79c6>&#34;size&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;long&#34;</span>
            },
            <span style=color:#ff79c6>&#34;upstream_addr&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            },
            <span style=color:#ff79c6>&#34;upstream_response_length&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;long&#34;</span>
            },
            <span style=color:#ff79c6>&#34;upstream_response_time&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;double&#34;</span>
            },
            <span style=color:#ff79c6>&#34;upstream_status&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            },
            <span style=color:#ff79c6>&#34;user&#34;</span>:{
               <span style=color:#ff79c6>&#34;type&#34;</span>:<span style=color:#f1fa8c>&#34;keyword&#34;</span>
            }
         }
      }
   }
}
</code></pre></div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-63db1ee76d05f13a8e7f9edb2a3df1f3>2.2.1 - Index Management</h1>
<div class=lead>Overview on supported OpenSearch index management scenarios</div>
<p>The helm chart maintains retention days for indices in OpenSearch using a <a href=https://opensearch.org/docs/latest/im-plugin/ism/index/>ISM policy</a> defined in file <a href=https://github.com/nickytd/kubernetes-logging-helm/blob/main/chart/index-templates/index-retention_policy.json><code>index-retention_policy.json</code></a>. Value is taken from <code>opensearch.retentionDays</code> key.</p>
<blockquote>
<p><strong>Note:</strong> Retention period configured in the helm chart (7 days by default) shall reflect the size of the persistence volumes mounted by the OpenSearch data nodes. If the logs volume in the cluster is high, the data nodes PVs sizes shall correspond.</p>
<p>It is a good practice to have a resizable storage class in the cluster supporting updates on the persistence volumes. When the persistence volumes fill up, the OpenSearch data node switch to read-only mode and new logs are prevented from indexing.</p>
</blockquote>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2fc083bd0278a17dc1d35efc939c811e>2.2.2 - Possible problems</h1>
<p>OpenSearch / ElasticSearch is pretty nice piece of technology with many self-healing procedure, but sometimes manual interventions is required. In this chapter you can find solutions for some problems.</p>
</div>
<div class=td-content>
<h1 id=pg-e4eb219fa55e4ace518681faad903db2>2.2.2.1 - Missing data node</h1>
<h1 id=prerequisites>Prerequisites</h1>
<p>In this guide I expect, that you have access to Kubernetes cluster and have portfoward to some <em>OpenSearch node</em> (= pod from Kubernetes perspective). I recommend chose one node <em>client</em> type.<br>
All API call to OpenSearch cluster beginning: <code>curl -ks https://&lt;Name>:&lt;Password>@localhost:9200/&lt;Source></code>, where:</p>
<ul>
<li><code>&lt;Name></code> = user name in OpenSearch cluster with admin privileges</li>
<li><code>&lt;Password></code> = corespondig password for admin account</li>
<li><code>&lt;Source></code> = datapoint from OpenSearch cluster</li>
</ul>
<ol>
<li>Check current status:</li>
</ol>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ curl -ks https://&lt;Name&gt;:&lt;Password&gt;@localhost:9200/_cat/health

<span style=color:#bd93f9>1654179496</span> 14:18:16 logging yellow <span style=color:#bd93f9>5</span> <span style=color:#bd93f9>1</span> <span style=color:#8be9fd;font-style:italic>true</span> <span style=color:#bd93f9>30</span> <span style=color:#bd93f9>30</span> <span style=color:#bd93f9>0</span> <span style=color:#bd93f9>0</span> <span style=color:#bd93f9>27</span> <span style=color:#bd93f9>0</span> - 52.6%
</code></pre></div><blockquote>
<p>our cluster is in <strong>yellow</strong> state</p>
</blockquote>
<ol start=2>
<li>List all available nodes from OpenSearch perspective:</li>
</ol>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ curl -ks https://&lt;Name&gt;:&lt;Password&gt;@localhost:9200/_cat/nodes
100.96.7.5  <span style=color:#bd93f9>72</span>  <span style=color:#bd93f9>97</span> <span style=color:#bd93f9>1</span> 0.30 0.36 0.35 mr  - ofd-manager-0
100.96.7.11 <span style=color:#bd93f9>51</span>  <span style=color:#bd93f9>76</span> <span style=color:#bd93f9>1</span> 0.30 0.36 0.35 r   - ofd-client-56dd9c66fb-bs7hp
100.96.7.7  <span style=color:#bd93f9>53</span>  <span style=color:#bd93f9>76</span> <span style=color:#bd93f9>4</span> 0.30 0.36 0.35 dir - ofd-data-1
100.96.1.8  <span style=color:#bd93f9>21</span> <span style=color:#bd93f9>100</span> <span style=color:#bd93f9>1</span> 1.73 0.82 0.41 mr  * ofd-manager-1
100.96.7.12 <span style=color:#bd93f9>19</span>  <span style=color:#bd93f9>76</span> <span style=color:#bd93f9>1</span> 0.30 0.36 0.35 r   - ofd-client-56dd9c66fb-q9tv5
</code></pre></div><blockquote>
<p>here you can see multinode setup, where must exist two nodes from type <em>client</em>, <em>data</em> and <em>manager</em> (6 OpenSearch nodes total)<br></p>
<p>one datanode &ldquo;ofd-data-0&rdquo; is missing</p>
</blockquote>
<ol start=3>
<li>Check suspicious pods:</li>
</ol>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ kubectl -n logging get pods | grep data
ofd-data-0    1/1    Running    <span style=color:#bd93f9>0</span>    130m
ofd-data-1    1/1    Running    <span style=color:#bd93f9>0</span>    129m
</code></pre></div><blockquote>
<p>from Kubernetes perspective this pods (OpenSearch node) working fine</p>
</blockquote>
<ol start=4>
<li>Check logs from suspicious pod:</li>
</ol>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ kubectl -n logging logs ofd-data-0
...
<span style=color:#f1fa8c>&#34;message&#34;</span>: <span style=color:#f1fa8c>&#34;failed to join ...
</span><span style=color:#f1fa8c>...
</span><span style=color:#f1fa8c>Caused by: org.opensearch.cluster.coordination.CoordinationStateRejectedException: join validation on cluster state with a different cluster uuid 2UlST0WBQIKEV05cDpuWwQ than local cluster uuid v1vi49Q_RRaaC83iMthBnQ, rejecting
</span><span style=color:#f1fa8c>...
</span></code></pre></div><blockquote>
<p>it seems, that this node hold old previously used OpenSearch cluster ID and attempt to connect to this old OpenSearch cluster instance</p>
<p><strong>this is reason, why this node is missing</strong></p>
</blockquote>
<ol start=5>
<li>Reset failed data node:
<div class="alert alert-warning" role=alert>
<h4 class=alert-heading>Warning</h4>
<strong>Double check, that you have at last half data nodes healthy!</strong> In our case, we must have 2 data nodes total and 1 is missing. <strong>Double check, that OpenSearch cluster is in yellow state. Proceeding with smaller amount of datanodes come to datalost!</strong>
</div>
</li>
</ol>
<ul>
<li>login to this pod:
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ kubectl -n logging <span style=color:#8be9fd;font-style:italic>exec</span> -i -t ofd-data-0 -- /bin/bash
</code></pre></div></li>
<li>delete datadir in this pod:
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ rm -rf /data/nodes
</code></pre></div></li>
<li>logout from this pod:
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ <span style=color:#8be9fd;font-style:italic>exit</span>
</code></pre></div></li>
<li>delete this pod for restarting:
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ k -n logging delete pod ofd-data-0
pod <span style=color:#f1fa8c>&#34;ofd-data-0&#34;</span> deleted
</code></pre></div></li>
</ul>
<ol start=6>
<li>Check OpenSearch cluster health again:</li>
</ol>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ curl -ks https://&lt;Name&gt;:&lt;Password&gt;@localhost:9200/_cat/health
<span style=color:#bd93f9>1654180648</span> 14:37:28 logging yellow <span style=color:#bd93f9>6</span> <span style=color:#bd93f9>2</span> <span style=color:#8be9fd;font-style:italic>true</span> <span style=color:#bd93f9>45</span> <span style=color:#bd93f9>30</span> <span style=color:#bd93f9>0</span> <span style=color:#bd93f9>4</span> <span style=color:#bd93f9>11</span> <span style=color:#bd93f9>0</span> - 75.0%
...
<span style=color:#bd93f9>1654180664</span> 14:37:44 logging yellow <span style=color:#bd93f9>6</span> <span style=color:#bd93f9>2</span> <span style=color:#8be9fd;font-style:italic>true</span> <span style=color:#bd93f9>53</span> <span style=color:#bd93f9>30</span> <span style=color:#bd93f9>0</span> <span style=color:#bd93f9>2</span> <span style=color:#bd93f9>5</span> <span style=color:#bd93f9>0</span> - 88.3%
...
<span style=color:#bd93f9>1654180852</span> 14:40:52 logging green <span style=color:#bd93f9>6</span> <span style=color:#bd93f9>2</span> <span style=color:#8be9fd;font-style:italic>true</span> <span style=color:#bd93f9>60</span> <span style=color:#bd93f9>30</span> <span style=color:#bd93f9>0</span> <span style=color:#bd93f9>0</span> <span style=color:#bd93f9>0</span> <span style=color:#bd93f9>0</span> - 100.0%
</code></pre></div><blockquote>
<p>our cluster is still in yellow state</p>
<p>running <code>curl</code> command over time give information, that cluster regenerating</p>
<p>wait some time and if this problem was solved, you can see cluster again healthy in <em>green</em> state</p>
</blockquote>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-64c562ddfdf1515623da37aa02133a86>2.3 - OpenSearch-Dashboards</h1>
<div class=lead>OpenSearch Dashboards configurations</div>
<p>Kubernetes logging helm chart deploys an single instance of <a href=https://opensearch.org/docs/latest/dashboards/quickstart-dashboards/>OpenSearch Dashboards</a> (or just Dashboards) presenting the UI interface to OpenSearch indices.</p>
<p>The helm chart enables authentication configurations based on SAML, ODIC or standalone and leverages dashboards tenant concept.
The latter allows teams to innovate UIs such as searches, visualizations and dashboards in shared tenant space leaving a predefined readonly UIs at a global space. Once the UIs are ready to be promoted those can become part of the helm chart <a href=https://github.com/nickytd/kubernetes-logging-helm/tree/main/chart/saved-objects>saved-objects</a> folder and become standard set of the chart deployment.</p>
<p><img src=./dashboards-tenants.jpg alt=dashboards></p>
<p>In addition the helm chart provisions an OpenSearch <a href=https://opensearch.org/docs/latest/data-prepper/index/>DataPrepper</a> component which allows OpenTelemetry traces to be indexed and later visualized at Dashboards observability UI.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-1c9f0942d4ab7bc96d6a676e1e85e588>2.3.1 - Opensearch-Dashboards Authentication & Authorization</h1>
<div class=lead>Opensearch Dashboards authentication & authorization configurations</div>
<p>tbd</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-b96842a864c9bbea35a261097c779f33>2.3.2 - OpenSearch-Dashboards-Observability</h1>
<div class=lead>OpenSearch Dashboards observability pluging</div>
<p>The <a href=https://opensearch.org/docs/latest/observing-your-data/trace/index/>Observability plugin</a> allow you to visualize tracing data.</p>
<p>Example:
<img src=./kubernetes-logging-opentelemetry.jpg alt=opentelemetry></p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-7fcc1ea9f1032bbae82f10dfb538a0cf>2.3.3 - Opensearch-Dashboards Visualizations</h1>
<div class=lead>Opensearch Dashboards visualizations configurations</div>
<p>tbd</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-3a8656ebec6b78171def1a3542380037>2.4 - Kafka</h1>
<div class=lead>Configuration settings for Kafka</div>
<p>Kubernetes logging helm chart deploys <a href=https://kafka.apache.org/>Apache Kafka</a> as a message broker between FluentBit and Logstash for improving stability and loadbalance in big deployments.</p>
<p>From helm chart version <strong>4.6.0</strong> we omited <a href=https://zookeeper.apache.org/>Apache ZooKeeper</a> as Kafka dependency. Kafka from version <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum><strong>2.8.0</strong></a> introduced <a href=https://developer.confluent.io/learn/kraft/><em>KRaft</em></a> aka ZooKeeper-less mode. From Kafka version <a href=https://cwiki.apache.org/confluence/display/KAFKA/KIP-833%3A+Mark+KRaft+as+Production+Ready><strong>3.3.0</strong></a> is KRaft marked as production ready, so, we decide to adopt it in the logging helm chart to save some resources and deploying time. Kafka in Raft mode need to have generated cluster ID. Please check <a href=https://pages.github.tools.sap/cs-devops/kubernetes-logging-helm/docs/components/kafka/howtos/clusterid/>how to generating cluster ID</a>.</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-febfba28b8eb6627e21e996fb64a46f8>2.4.1 - Howtos</h1>
<div class=lead>How to achieve &mldr;</div>
<p>Here you can find guidelines, how to manipulate with Kafka in context of logging helm chart.</p>
</div>
<div class=td-content>
<h1 id=pg-d2aa4f1d32e931ed2096133d7d8c74c8>2.4.1.1 - How to generating cluster ID</h1>
<div class=lead>How to generating cluster ID</div>
<p>Cluster ID is required for each Kafka instance to know, to which instance it must to connect and make cluster. It is also used to <a href=https://kafka.apache.org/documentation/#kraft_storage>preparing storage space</a>. We need to set all Kafka instance same cluster ID. If you omit this settings, all Kafka instance generate their own ID and reject make cluster.</p>
<p>There is many ways, how to generate the ID, but I recommend use this chain of Bash command:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ cat /proc/sys/kernel/random/uuid | tr -d <span style=color:#f1fa8c>&#39;-&#39;</span> | base64 | cut -b 1-22
</code></pre></div><p>You can also use <a href=https://kafka.apache.org/33/documentation.html#quickstart_startserver>built-in script</a>:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>$ bin/kafka-storage.sh random-uuid
</code></pre></div><p>Or just start one Kafka instance without the cluster ID settings and it will be generated to the logs.</p>
<p><em>Sources</em>:</p>
<p><a href=https://sleeplessbeastie.eu/2021/10/22/how-to-generate-kafka-cluster-id/>Blog sleeplessbeastie.eu</a><br>
<a href=https://kafka.apache.org/33/documentation.html>Apache Kafka Documentation</a></p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-5cb0fd650727bc1f9017ddcace2f3c54>3 - Upgrade Notes</h1>
<h1 id=2x---300>2.x -> 3.0.0</h1>
<p>Since version 3.0.0, the chart values are renamed and follow <a href=https://helm.sh/docs/chart_best_practices/values/>camel case recommendation</a>. This is a backward <strong>incompatibility change</strong> and helm chart values for releases needs first to be migrated to the recommended camel case format.</p>
<h1 id=454---460>4.5.4 -> 4.6.0</h1>
<p>In the version <strong>4.6.0</strong> we omited <a href=https://zookeeper.apache.org/>Apache ZooKeeper</a> as Kafka dependency. In Raft mode Kafka cluster need to have generated cluster ID. Please check <a href=https://nickytd.github.io/kubernetes-logging-helm/docs/components/kafka/howtos/clusterid/>how to generating cluster ID</a> and change your <code>values.yaml</code> file accordingly.</p>
<p>If you dont want lost any of your log data in upgrading, here is a safe procedure:</p>
<ul>
<li>delete FluentBit daemonset (this stop feeding Kafka with new log data)</li>
<li>wait, until Logstash process all cached log records from Kafka to OpenSearch (checking is possible via monitoring API or in Grafana Dashboard)</li>
<li>scale down Kafka StatefulSet to zero (this delete old Kafka implementation)</li>
<li>delete Kafka StatefulSet</li>
<li>scale down ZooKeeper StatefulSet to zero</li>
<li>delete ZooKeeper StatefulSet</li>
<li>delete PersistantVolumeClaim for Kafka&rsquo;s and for ZooKeeper&rsquo;s instances as well</li>
<li>do <code>helm upgrade ...</code></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-4553fe28b20f652961f013e3fc49ad6a>4 - Helm chart values</h1>
<div class=lead>Helm chart values description</div>
<p>Bellow you can find <a href=https://github.com/nickytd/kubernetes-logging-helm/blob/main/chart/values.yaml>helm chart values</a> usage with description.</p>
<p><img src="https://img.shields.io/badge/Version-4.6.3-informational?style=flat-square" alt="Version: 4.6.3"></p>
<h2 id=values>Values</h2>
<table>
<thead>
<tr>
<th>Key</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>additionalJobAnnotations</td>
<td>object</td>
<td><code>{}</code></td>
<td>Additional annotations for jobs pods</td>
</tr>
<tr>
<td>additionalJobPodAnnotations</td>
<td>string</td>
<td><code>nil</code></td>
<td>Additional annotations for job pods</td>
</tr>
<tr>
<td>additionalJobPodLabels</td>
<td>object</td>
<td><code>{}</code></td>
<td>Additional labels for job pods</td>
</tr>
<tr>
<td>client.affinity</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>client.heapSize</td>
<td>string</td>
<td><code>"512M"</code></td>
<td></td>
</tr>
<tr>
<td>client.ingress.annotations</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>client.ingress.className</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>client.ingress.enabled</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>client.ingress.host</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>client.ingress.path</td>
<td>string</td>
<td><code>"/"</code></td>
<td></td>
</tr>
<tr>
<td>client.ingress.tls</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>client.podLabels</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>client.priorityClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>client.replicas</td>
<td>int</td>
<td><code>1</code></td>
<td></td>
</tr>
<tr>
<td>client.resources.limits.memory</td>
<td>string</td>
<td><code>"2000Mi"</code></td>
<td></td>
</tr>
<tr>
<td>client.resources.requests.memory</td>
<td>string</td>
<td><code>"1000Mi"</code></td>
<td></td>
</tr>
<tr>
<td>client.tolerations</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>client.topologySpreadConstraints</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>clusterManager.affinity</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>clusterManager.heapSize</td>
<td>string</td>
<td><code>"256M"</code></td>
<td></td>
</tr>
<tr>
<td>clusterManager.podLabels</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>clusterManager.priorityClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>clusterManager.replicas</td>
<td>int</td>
<td><code>1</code></td>
<td></td>
</tr>
<tr>
<td>clusterManager.resources.limits.memory</td>
<td>string</td>
<td><code>"700Mi"</code></td>
<td></td>
</tr>
<tr>
<td>clusterManager.resources.requests.memory</td>
<td>string</td>
<td><code>"700Mi"</code></td>
<td></td>
</tr>
<tr>
<td>clusterManager.storage</td>
<td>string</td>
<td><code>"1Gi"</code></td>
<td></td>
</tr>
<tr>
<td>clusterManager.storageClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>clusterManager.tolerations</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>clusterName</td>
<td>string</td>
<td><code>"logging"</code></td>
<td>Default cluster name.</td>
</tr>
<tr>
<td>data.affinity</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>data.heapSize</td>
<td>string</td>
<td><code>"512M"</code></td>
<td></td>
</tr>
<tr>
<td>data.podLabels</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>data.priorityClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>data.replicas</td>
<td>int</td>
<td><code>1</code></td>
<td></td>
</tr>
<tr>
<td>data.resources.limits.memory</td>
<td>string</td>
<td><code>"2000Mi"</code></td>
<td></td>
</tr>
<tr>
<td>data.resources.requests.memory</td>
<td>string</td>
<td><code>"1000Mi"</code></td>
<td></td>
</tr>
<tr>
<td>data.storage</td>
<td>string</td>
<td><code>"1Gi"</code></td>
<td></td>
</tr>
<tr>
<td>data.storageClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>data.tolerations</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.affinity</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.enabled</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.heapSize</td>
<td>string</td>
<td><code>"256M"</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.image</td>
<td>string</td>
<td><code>"opensearchproject/data-prepper"</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.imageTag</td>
<td>string</td>
<td><code>"2.0.1"</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.podLabels</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.priorityClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.replicas</td>
<td>int</td>
<td><code>1</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.resources.limits.memory</td>
<td>string</td>
<td><code>"600Mi"</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.resources.requests.memory</td>
<td>string</td>
<td><code>"600Mi"</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.retention.purge</td>
<td>int</td>
<td><code>3</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.retention.slotSize</td>
<td>int</td>
<td><code>10</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.tolerations</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>data_prepper.topologySpreadConstraints</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.affinity</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.caCertificateSecret</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.containersLogsHostPath</td>
<td>string</td>
<td><code>"/var/log/pods"</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.containersRuntime</td>
<td>string</td>
<td><code>"containerd"</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.disableTailInput</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.enabled</td>
<td>bool</td>
<td><code>true</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.extraEnvs</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.image</td>
<td>string</td>
<td><code>"fluent/fluent-bit"</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.imagePullPolicy</td>
<td>string</td>
<td><code>"IfNotPresent"</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.imageTag</td>
<td>string</td>
<td><code>"2.0.9"</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.indexPrefix</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.journalsLogsHostPath</td>
<td>string</td>
<td><code>"/var/log"</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.mergeLog</td>
<td>string</td>
<td><code>"On"</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.metrics.enabled</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.metrics.interval</td>
<td>string</td>
<td><code>"30s"</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.metrics.namespace</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.podLabels</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.priorityClass</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.readFromHead</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.resources.limits.memory</td>
<td>string</td>
<td><code>"100Mi"</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.resources.requests.memory</td>
<td>string</td>
<td><code>"50Mi"</code></td>
<td></td>
</tr>
<tr>
<td>fluentbit.tolerations[0].operator</td>
<td>string</td>
<td><code>"Exists"</code></td>
<td></td>
</tr>
<tr>
<td>imagePullSecrets</td>
<td>list</td>
<td><code>[]</code></td>
<td>Secrets containing credentials for pulling images from private registers</td>
</tr>
<tr>
<td>init_container.image</td>
<td>string</td>
<td><code>"nickytd/init-container"</code></td>
<td></td>
</tr>
<tr>
<td>init_container.imagePullPolicy</td>
<td>string</td>
<td><code>"IfNotPresent"</code></td>
<td></td>
</tr>
<tr>
<td>init_container.imageTag</td>
<td>string</td>
<td><code>"1.0.5"</code></td>
<td></td>
</tr>
<tr>
<td>kafka.affinity</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>kafka.enabled</td>
<td>bool</td>
<td><code>true</code></td>
<td></td>
</tr>
<tr>
<td>kafka.heapSize</td>
<td>string</td>
<td><code>"256M"</code></td>
<td></td>
</tr>
<tr>
<td>kafka.image</td>
<td>string</td>
<td><code>"bitnami/kafka"</code></td>
<td></td>
</tr>
<tr>
<td>kafka.imageTag</td>
<td>string</td>
<td><code>"3.4.0"</code></td>
<td></td>
</tr>
<tr>
<td>kafka.kraftId</td>
<td>string</td>
<td><code>"M2M5NGQ3ZDA5NWI1NDkxYz"</code></td>
<td></td>
</tr>
<tr>
<td>kafka.podLabels</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>kafka.priorityClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>kafka.replicas</td>
<td>int</td>
<td><code>1</code></td>
<td></td>
</tr>
<tr>
<td>kafka.resources.limits.memory</td>
<td>string</td>
<td><code>"600Mi"</code></td>
<td></td>
</tr>
<tr>
<td>kafka.resources.requests.memory</td>
<td>string</td>
<td><code>"600Mi"</code></td>
<td></td>
</tr>
<tr>
<td>kafka.storage</td>
<td>string</td>
<td><code>"10Gi"</code></td>
<td></td>
</tr>
<tr>
<td>kafka.storageClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>kafka.tolerations</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>kafka.topics[0].config</td>
<td>string</td>
<td><code>"max.message.bytes=10000000,retention.bytes=-1,retention.ms=3600000"</code></td>
<td></td>
</tr>
<tr>
<td>kafka.topics[0].name</td>
<td>string</td>
<td><code>"containers"</code></td>
<td></td>
</tr>
<tr>
<td>logstash.affinity</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>logstash.enabled</td>
<td>bool</td>
<td><code>true</code></td>
<td></td>
</tr>
<tr>
<td>logstash.heapSize</td>
<td>string</td>
<td><code>"256M"</code></td>
<td></td>
</tr>
<tr>
<td>logstash.image</td>
<td>string</td>
<td><code>"opensearchproject/logstash-oss-with-opensearch-output-plugin"</code></td>
<td></td>
</tr>
<tr>
<td>logstash.imageTag</td>
<td>string</td>
<td><code>"8.6.1"</code></td>
<td></td>
</tr>
<tr>
<td>logstash.monitoring.enabled</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>logstash.monitoring.image</td>
<td>string</td>
<td><code>"nickytd/logstash-exporter"</code></td>
<td></td>
</tr>
<tr>
<td>logstash.monitoring.imageTag</td>
<td>string</td>
<td><code>"0.3.0"</code></td>
<td></td>
</tr>
<tr>
<td>logstash.monitoring.metricsPort</td>
<td>int</td>
<td><code>9198</code></td>
<td></td>
</tr>
<tr>
<td>logstash.monitoring.serviceMonitor.enabled</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>logstash.monitoring.serviceMonitor.namespace</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>logstash.podLabels</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>logstash.priorityClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>logstash.replicas</td>
<td>int</td>
<td><code>1</code></td>
<td></td>
</tr>
<tr>
<td>logstash.resources.limits.memory</td>
<td>string</td>
<td><code>"700Mi"</code></td>
<td></td>
</tr>
<tr>
<td>logstash.resources.requests.memory</td>
<td>string</td>
<td><code>"700Mi"</code></td>
<td></td>
</tr>
<tr>
<td>logstash.tolerations</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>logstash.topologySpreadConstraints</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.additionalJvmParams</td>
<td>string</td>
<td><code>"-Djava.net.preferIPv4Stack=true -XshowSettings:properties -XshowSettings:vm -XshowSettings:system"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.certManager.enabled</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.certManager.issuerRef</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.certManager.namespace</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.externalOpensearch.disabled</td>
<td>bool</td>
<td><code>true</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.externalOpensearch.url</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.image</td>
<td>string</td>
<td><code>"opensearchproject/opensearch"</code></td>
<td>Used image name.</td>
</tr>
<tr>
<td>opensearch.imagePullPolicy</td>
<td>string</td>
<td><code>"IfNotPresent"</code></td>
<td>Image pull <a href=https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy>policy</a>.</td>
</tr>
<tr>
<td>opensearch.imageTag</td>
<td>string</td>
<td><code>"2.6.0"</code></td>
<td>Used component version.</td>
</tr>
<tr>
<td>opensearch.oidc</td>
<td>object</td>
<td><code>(see example in values file)</code></td>
<td>Place here your settings, if you want to authenticate via <em>OIDC</em> method.</td>
</tr>
<tr>
<td>opensearch.password</td>
<td>string</td>
<td><code>"osadmin"</code></td>
<td>Password for account with admin rights</td>
</tr>
<tr>
<td>opensearch.retentionDays</td>
<td>int</td>
<td><code>7</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.saml</td>
<td>object</td>
<td><code>(see example in values file)</code></td>
<td>Place here your settings, if you want to authenticate via <em>SAML</em> method.</td>
</tr>
<tr>
<td>opensearch.singleNode</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.snapshot.enabled</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.snapshot.size</td>
<td>string</td>
<td><code>"5Gi"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.snapshot.storageClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.timeNanoSeconds</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>opensearch.user</td>
<td>string</td>
<td><code>"osadmin"</code></td>
<td>User name with admin rights</td>
</tr>
<tr>
<td>opensearch_dashboards.affinity</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.branding</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.developer.password</td>
<td>string</td>
<td><code>"develop"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.developer.user</td>
<td>string</td>
<td><code>"developer"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.externalOpensearchDashboards.caCertificateSecret</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.externalOpensearchDashboards.disabled</td>
<td>bool</td>
<td><code>true</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.externalOpensearchDashboards.runJob</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.externalOpensearchDashboards.url</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.extraEnvs</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.image</td>
<td>string</td>
<td><code>"opensearchproject/opensearch-dashboards"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.imageTag</td>
<td>string</td>
<td><code>"2.6.0"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.indexPatterns[0]</td>
<td>string</td>
<td><code>"containers"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.indexPatterns[1]</td>
<td>string</td>
<td><code>"systemd"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.indexPatterns[2]</td>
<td>string</td>
<td><code>"nginx"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.ingress.annotations</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.ingress.className</td>
<td>string</td>
<td><code>""</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.ingress.enabled</td>
<td>bool</td>
<td><code>false</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.ingress.host</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.ingress.hosts</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.ingress.path</td>
<td>string</td>
<td><code>"/"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.ingress.tls</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.password</td>
<td>string</td>
<td><code>"opensearch"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.podLabels</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.priorityClass</td>
<td>object</td>
<td><code>{}</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.readonly.password</td>
<td>string</td>
<td><code>"view"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.readonly.user</td>
<td>string</td>
<td><code>"viewer"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.replicas</td>
<td>int</td>
<td><code>1</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.resources.limits.memory</td>
<td>string</td>
<td><code>"500Mi"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.resources.requests.memory</td>
<td>string</td>
<td><code>"500Mi"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.tenants[0]</td>
<td>string</td>
<td><code>"Global"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.tenants[1]</td>
<td>string</td>
<td><code>"Developer"</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.tolerations</td>
<td>list</td>
<td><code>[]</code></td>
<td></td>
</tr>
<tr>
<td>opensearch_dashboards.user</td>
<td>string</td>
<td><code>"opensearch"</code></td>
<td></td>
</tr>
<tr>
<td>priorityClass</td>
<td>string</td>
<td><code>""</code></td>
<td>TODO</td>
</tr>
<tr>
<td>storageClass</td>
<td>object</td>
<td><code>{}</code></td>
<td>Defautl Storage Class for used by Persistence Volume Claims. Can be overwritten by workloads</td>
</tr>
<tr>
<td>withNetworkPolicy</td>
<td>bool</td>
<td><code>false</code></td>
<td>Default networkpolicy for ingress and egress traffic</td>
</tr>
</tbody>
</table>
<h2 id=maintainers>Maintainers</h2>
<table>
<thead>
<tr>
<th>Name</th>
<th>Email</th>
<th>Url</th>
</tr>
</thead>
<tbody>
<tr>
<td>Niki Dokovski</td>
<td><a href=mailto:nickytd@gmail.com>nickytd@gmail.com</a></td>
<td><a href=https://github.com/nickytd>https://github.com/nickytd</a></td>
</tr>
</tbody>
</table>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-4588103fb9a692533044ee756a5f863f>5 - Frequently asked questions</h1>
<p>#TODO</p>
</div>
</main>
</div>
</div>
<footer class="bg-dark py-5 row d-print-none">
<div class="container-fluid mx-sm-5">
<div class=row>
<div class="col-6 col-sm-4 text-xs-center order-sm-2">
</div>
<div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
</div>
<div class="col-12 col-sm-4 text-center py-2 order-sm-2">
</div>
</div>
</div>
</footer>
</div>
<script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/kubernetes-logging-helm/js/tabpane-persist.js></script>
<script src=/kubernetes-logging-helm/js/main.min.8ab8f81ff7e1454d30024cd6f956d4d341c3a97e2a673f988065f2ee4e147922.js integrity="sha256-irj4H/fhRU0wAkzW+VbU00HDqX4qZz+YgGXy7k4UeSI=" crossorigin=anonymous></script>
</body>
</html>