[{"body":"This helm chart deploys a scalable containerized logging stack with the main purpose of enabling log observability for kubernetes applications. The design supports both local development use cases such as minikube deployments up to a scaled production scenarios. The log shippers are FluentBits deployed on each kubernetes node mounting the host filesystem. The latter scenarios leverage Kafka message broker, completely decoupling in this way, the log generation and log indexing functions.\nThe helm chart supports OpenSearch in various configurations starting from a single node setup usable for local development, to a scaled multi nodes OpenSearch deployment suitable for production environment. In the latter case there are 3 types of nodes (coordination, data and master) where each of those can be both horizontally and vertically scaled depending on the load and shards replication demands.\nFinally this helm chart provides index templates management in OpenSearch and index pattern management in OpenSearchDashboards. An initial predefined set of dashboards is also provided for illustration purposes.\nAdding the helm chart repository: helm repo add logging https://nickytd.github.io/kubernetes-logging-helm helm repo update  Note Any authenticated user should have read access to the helm repository.  Prepare a release configuration The recommended approach is the get the default helm chart values and adjust accordingly. At minimum the ingress annotations for the OpenSearch rest endpoint and OpenSearchDashboards UI app have to be adjusted. Here is an example for a minimal single OpenSearch node setup.\nInstall a release helm install ofd logging/kubernetes-logging ","categories":"","description":"","excerpt":"This helm chart deploys a scalable containerized logging stack with …","ref":"/kubernetes-logging-helm/docs/","tags":"","title":"Helm chart overview"},{"body":"The kubernetes logging helm chart supports a number of deployment layouts of OpenSearch and other components depending on the concrete purpose and size of the cluster.\nSingle node OpenSearch opensearch: single_node: true fluentbit: containersLogsHostPath: /var/log/pods journalsLogsHostPath: /var/log containersRuntime: docker kafka: false: This layout is the simplest possible requiring the least compute and memory resources. It comprises of the log shippers, a single OpenSearch node and a single OpenSearchDashabord UI. The log shippers are FluentBits deployed on each kubernetes node mounting the host filesystem. Because the locations of the containers logs or the host journals can vary, those locations have to be adapted accordingly in the FluentBit configuration. The logs are directly send to the Opensearch node for indexing without the need of a message broker in between.\n Recommendation: Although the single node can be scaled by simply increasing the replicas in the “data” configuration, this setup is most suitable for development environments like minikube or kind clusters.\n Multi node OpenSearch OpenSearch supports dedicated node types based on specific functions in the cluster. A coordination node, data node and master node forming an OpenSearch cluster can be deployed when single_node option is disabled.\nopensearch: single_node: false Scaled multi node OpenSearch in production When the setup is deployed in a production environment both aspects for reliably and throughout of the logs streams are addressed by the helm chart with the introduction of a message broker. A running message broker (Kafka) effectively accumulates spikes of logs volumes or downtimes of the backend OpenSearch cluster.\nNote: Kafka and Logstash needs to be enabled as well!.   Delivery chain is: Kafka -\u003e Logstash -\u003e OpenSearch\n Even more importantly each component can be scaled horizontally insuring better reliability.\nopensearch: single_node: false data: replicas: 3 master: replicas: 3 client: replicas: 3 kafka: enabled: true replicas: 3 zookeeper: replicas: 3 fluentd: replicas: 3 #and so on Additionally each type of workload scheduling strategy can be further optimized by defining node and pods (anti)affinity rules.\nFor example for stateful sets like Kafka’s or data nodes following affinity strategy guarantees that pods will be scheduled on different kubernetes nodes.\naffinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: type operator: In values: # with the corresponding label - kafka topologyKey: kubernetes.io/hostname Or in the case of a deployments like the OpenSearch coordination nodes or Logstashes a spread of pods over nodes can be achieved with:\ntopologySpreadConstraints: - maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: # with the corresponding label type: client ","categories":"","description":"Overview on available installation layouts\n","excerpt":"Overview on available installation layouts\n","ref":"/kubernetes-logging-helm/docs/deployments/","tags":"","title":"Deployment Layouts"},{"body":"2.x -\u003e 3.0.0 Since version 3.0.0, the chart values are renamed and follow camel case recommendation. This is a backward incompatibility change and helm chart values for releases needs first to be migrated to the recommended camel case format.\n","categories":"","description":"","excerpt":"2.x -\u003e 3.0.0 Since version 3.0.0, the chart values are renamed and …","ref":"/kubernetes-logging-helm/docs/upgrade-notes/","tags":"","title":"Upgrade Notes"},{"body":"Bellow you can find helm chart values usage with description.\nValues    Key Type Default Description     additionalJobAnnotations object {} Additional annotations for chart jobs.   client.affinity object {}    client.heapSize string \"512M\"    client.ingress.annotations object {}    client.ingress.className string \"\"    client.ingress.enabled bool false    client.ingress.host list []    client.ingress.path string \"/\"    client.ingress.tls list []    client.priorityClass object {}    client.replicas int 1    client.resources.limits.memory string \"2000Mi\"    client.resources.requests.memory string \"1000Mi\"    client.tolerations list []    client.topologySpreadConstraints object {}    clusterName string \"logging\" Default cluster name.   data.affinity object {}    data.heapSize string \"512M\"    data.priorityClass object {}    data.replicas int 1    data.resources.limits.memory string \"2000Mi\"    data.resources.requests.memory string \"1000Mi\"    data.storage string \"1Gi\"    data.storageClass object {}    data.tolerations list []    data_prepper.affinity object {}    data_prepper.enabled bool false    data_prepper.heapSize string \"256M\"    data_prepper.image string \"opensearchproject/data-prepper\"    data_prepper.imageTag string \"1.3.0\"    data_prepper.priorityClass object {}    data_prepper.replicas int 1    data_prepper.resources.limits.memory string \"600Mi\"    data_prepper.resources.requests.memory string \"600Mi\"    data_prepper.tolerations list []    data_prepper.topologySpreadConstraints object {}    fluentbit.affinity object {}    fluentbit.containersLogsHostPath string \"/var/log/pods\"    fluentbit.containersRuntime string \"docker\"    fluentbit.extraEnvs object {}    fluentbit.image string \"fluent/fluent-bit\"    fluentbit.imagePullPolicy string \"IfNotPresent\"    fluentbit.imageTag string \"1.8.15\"    fluentbit.journalsLogsHostPath string \"/var/log\"    fluentbit.metrics.enabled bool false    fluentbit.metrics.interval string \"30s\"    fluentbit.metrics.namespace object {}    fluentbit.priorityClass object {}    fluentbit.resources.limits.memory string \"100Mi\"    fluentbit.resources.requests.memory string \"50Mi\"    fluentbit.tolerations[0].operator string \"Exists\"    fluentbitConfigWatcher.image string \"nickytd/config-watcher\"    fluentbitConfigWatcher.imagePullPolicy string \"IfNotPresent\"    fluentbitConfigWatcher.imageTag string \"0.1.0\"    imagePullSecrets list [] Secrets containing credentials for pulling images from private registers   init_container.image string \"nickytd/init-container\"    init_container.imagePullPolicy string \"IfNotPresent\"    init_container.imageTag string \"1.0.2\"    kafka.affinity object {}    kafka.enabled bool true    kafka.heapSize string \"256M\"    kafka.image string \"bitnami/kafka\"    kafka.imageTag string \"3.1.0\"    kafka.priorityClass object {}    kafka.replicas int 1    kafka.resources.limits.memory string \"600Mi\"    kafka.resources.requests.memory string \"600Mi\"    kafka.storage string \"1Gi\"    kafka.storageClass object {}    kafka.tolerations list []    kafka.topics.config string \"max.message.bytes=10000000,retention.bytes=134217728,retention.ms=3600000,message.timestamp.difference.max.ms=3600000,message.timestamp.type=LogAppendTime\"    kafka.topics.name[0] string \"containers\"    logstash.affinity object {}    logstash.enabled bool true    logstash.heapSize string \"256M\"    logstash.image string \"opensearchproject/logstash-oss-with-opensearch-output-plugin\"    logstash.imageTag string \"7.16.3\"    logstash.monitoring.enabled bool false    logstash.monitoring.image string \"nickytd/logstash-exporter\"    logstash.monitoring.imageTag string \"0.3.0\"    logstash.monitoring.metricsPort int 9198    logstash.monitoring.serviceMonitor.enabled bool false    logstash.monitoring.serviceMonitor.namespace string \"\"    logstash.priorityClass object {}    logstash.replicas int 1    logstash.resources.limits.memory string \"700Mi\"    logstash.resources.requests.memory string \"700Mi\"    logstash.tolerations list []    logstash.topologySpreadConstraints object {}    master.affinity object {}    master.heapSize string \"256M\"    master.priorityClass object {}    master.replicas int 1    master.resources.limits.memory string \"600Mi\"    master.resources.requests.memory string \"600Mi\"    master.storage string \"1Gi\"    master.storageClass object {}    master.tolerations list []    opensearch.additionalJvmParams string \"-Djava.net.preferIPv4Stack=true -XshowSettings:properties -XshowSettings:vm -XshowSettings:system\"    opensearch.image string \"opensearchproject/opensearch\"    opensearch.imagePullPolicy string \"IfNotPresent\"    opensearch.imageTag string \"1.3.0\"    opensearch.inCluster bool true    opensearch.inClusterCertificates.generateCertificates bool true    opensearch.inClusterCertificates.secretName object {}    opensearch.oidc object (see example in values file) Place here your settings, if you want to authenticate via OIDC method.   opensearch.password string \"osadmin\"    opensearch.port int 9200    opensearch.retentionDays int 7    opensearch.saml object (see example in values file) Place here your settings, if you want to authenticate via SAML method.   opensearch.singleNode bool false    opensearch.snapshot.enabled bool false    opensearch.snapshot.size string \"5Gi\"    opensearch.snapshot.storageClass object {}    opensearch.url object {}    opensearch.user string \"osadmin\"    opensearch_dashboards.affinity object {}    opensearch_dashboards.developer.password string \"develop\"    opensearch_dashboards.developer.user string \"developer\"    opensearch_dashboards.extraEnvs[0].name string \"NODE_OPTIONS\"    opensearch_dashboards.extraEnvs[0].value string \"--max-old-space-size=350\"    opensearch_dashboards.image string \"opensearchproject/opensearch-dashboards\"    opensearch_dashboards.imageTag string \"1.3.0\"    opensearch_dashboards.inCluster bool true    opensearch_dashboards.indexPatterns[0] string \"containers\"    opensearch_dashboards.indexPatterns[1] string \"systemd\"    opensearch_dashboards.indexPatterns[2] string \"nginx\"    opensearch_dashboards.ingress.annotations object {}    opensearch_dashboards.ingress.className string \"\"    opensearch_dashboards.ingress.enabled bool false    opensearch_dashboards.ingress.host list []    opensearch_dashboards.ingress.hosts object {}    opensearch_dashboards.ingress.path string \"/\"    opensearch_dashboards.ingress.tls list []    opensearch_dashboards.password string \"opensearch\"    opensearch_dashboards.priorityClass object {}    opensearch_dashboards.readonly.password string \"view\"    opensearch_dashboards.readonly.user string \"viewer\"    opensearch_dashboards.replicas int 1    opensearch_dashboards.resources.limits.memory string \"500Mi\"    opensearch_dashboards.resources.requests.memory string \"500Mi\"    opensearch_dashboards.tenants[0] string \"Global\"    opensearch_dashboards.tenants[1] string \"Developer\"    opensearch_dashboards.tolerations list []    opensearch_dashboards.url object {}    opensearch_dashboards.user string \"opensearch\"    os_curator.image string \"nickytd/os-curator\"    os_curator.imagePullPolicy string \"IfNotPresent\"    os_curator.imageTag string \"5.8.4\"    priorityClass string \"logging\" TODO   storageClass object {} Defautl Storage Class for used by Persistence Volume Claims. Can be overwritten by workloads   zookeeper.affinity object {}    zookeeper.heapSize string \"128M\"    zookeeper.image string \"zookeeper\"    zookeeper.imageTag string \"3.8.0\"    zookeeper.priorityClass object {}    zookeeper.replicas int 1    zookeeper.resources.limits.memory string \"300Mi\"    zookeeper.resources.requests.memory string \"300Mi\"    zookeeper.storage string \"1Gi\"    zookeeper.storageClass object {}    zookeeper.tolerations list []     Maintainers    Name Email Url     Niki Dokovski nickytd@gmail.com https://github.com/nickytd    ","categories":"","description":"Helm chart values description\n","excerpt":"Helm chart values description\n","ref":"/kubernetes-logging-helm/docs/chart-values/","tags":"","title":"Helm chart values"},{"body":"#TODO\n","categories":"","description":"","excerpt":"#TODO\n","ref":"/kubernetes-logging-helm/docs/faq/","tags":"","title":"Frequently asked questions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-logging-helm/categories/","tags":"","title":"Categories"},{"body":" Welcome to OFD logging stack project! -= A scalable containerized logging stack featuring Opensearch for kubernetes clusters =-\nDocumentation   Source Repository          ","categories":"","description":"","excerpt":" Welcome to OFD logging stack project! -= A scalable containerized …","ref":"/kubernetes-logging-helm/","tags":"","title":"HomePage"},{"body":"","categories":"","description":"","excerpt":"","ref":"/kubernetes-logging-helm/tags/","tags":"","title":"Tags"}]