{{- if and (.Values.data_prepper) (.Values.data_prepper.enabled) }}
apiVersion: v1
kind: Secret
metadata:
  name: {{ .Release.Name }}-data-prepper
  labels: {{ include "logging.labels" . | indent 4 }}
stringData:
  pipelines.yaml: |-
    entry-pipeline:
      workers: 4
      delay: "100"
      source:
        otel_trace_source:
          ssl: true
          sslKeyCertChainFile: "/usr/share/data-prepper/server.crt"
          sslKeyFile: "/usr/share/data-prepper/server.key"
          port: 21890
      buffer:
        bounded_blocking:
          buffer_size: 1024 # max number of records the buffer accepts
          batch_size: 256 # max number of records the buffer drains after each read
      sink:
        - pipeline:
            name: "raw-pipeline"
        - pipeline:
            name: "service-map-pipeline"

    raw-pipeline:
      source:
        pipeline:
          name: "entry-pipeline"
      buffer:
        bounded_blocking:
          # Configure the same value as in otel-trace-pipeline
          # Make sure you configure sufficient heap
          # default value is 512
          buffer_size: 1024
          # The raw processor does bulk request to your OpenSearch sink, so configure the batch_size higher.
          # If you use the recommended otel-collector setup each ExportTraceRequest could contain max 50 spans. https://github.com/opensearch-project/data-prepper/tree/v0.7.x/deployment/aws
          # With 64 as batch size each worker thread could process upto 3200 spans (64 * 50)
          batch_size: 256
      processor:
        - otel_trace_raw:
      sink:
        - opensearch:
            index_type: custom
            index: otel-v1-apm-span
            hosts: [ {{ include "os_url" . }} ]
            cert: "/usr/share/data-prepper/root-ca.pem"
            template_file: "/usr/share/data-prepper/otel-span-index-template.json"
            ism_policy_file: "/usr/share/data-prepper/otel-span-ism-policy.json"
            username: {{ .Values.opensearch.user }}
            password: {{ .Values.opensearch.password }}
    service-map-pipeline:
      delay: "100"
      source:
        pipeline:
          name: "entry-pipeline"
      processor:
        - service_map_stateful:
          # The window duration is the maximum length of time the data prepper stores the most recent trace data to evaluvate service-map relationships.
          # The default is 3 minutes, this means we can detect relationships between services from spans reported in last 3 minutes.
          # Set higher value if your applications have higher latency.
          window_duration: 180
      buffer:
        bounded_blocking:
          # Configure the same value as in otel-trace-pipeline
          # Make sure you configure sufficient heap
          # default value is 512
          buffer_size: 1024
          # The raw processor does bulk request to your OpenSearch sink, so configure the batch_size higher.
          # If you use the recommended otel-collector setup each ExportTraceRequest could contain max 50 spans. https://github.com/opensearch-project/data-prepper/tree/v0.7.x/deployment/aws
          # With 64 as batch size each worker thread could process upto 3200 spans (64 * 50)
          batch_size: 256
      sink:
        - opensearch:
            hosts: [ {{ include "os_url" . }} ]
            cert: "/usr/share/data-prepper/root-ca.pem"
            username: {{ .Values.opensearch.user }}
            password: {{ .Values.opensearch.password }}
            index_type: trace-analytics-service-map
{{- end }}
