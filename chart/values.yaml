### Default values for kubernetes-logging

# All values with description you can find
# https://nickytd.github.io/kubernetes-logging-helm/docs/chart-values/

# Global values
# -- Default cluster name.
clusterName: "logging"
# -- Secrets containing credentials for pulling images from private registers
imagePullSecrets: []
# -- TODO
priorityClass: ""
# -- Defautl Storage Class for used by Persistence Volume Claims. Can be overwritten by workloads
storageClass: {}
# -- Default networkpolicy for ingress and egress traffic
withNetworkPolicy: false

### OFD configuration ###
opensearch:
  # -- Used image name.
  image: "opensearchproject/opensearch"
  # -- Used component version.
  imageTag: 2.4.0
  # -- Image pull [policy](https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy).
  imagePullPolicy: IfNotPresent
  singleNode: false
  # externalOpensearch.disabled designates an external managed opensearch stack.
  externalOpensearch:
    disabled: true
    url: ""
  snapshot:
    enabled: false
    storageClass: {}
    size: "5Gi"
  retentionDays: 7
  additionalJvmParams: "-Djava.net.preferIPv4Stack=true -XshowSettings:properties -XshowSettings:vm -XshowSettings:system"
  # -- User name with admin rights
  user: "osadmin"
  # -- Password for account with admin rights
  password: "osadmin"
  certManager:
    enabled: false
    namespace: ""
    issuerRef: {}
    # kind:
    # name:
  # -- Place here your settings, if you want to authenticate via *SAML* method.
  # @default -- `(see example in values file)`
  saml: {}
  #  enabled: false
  #  idp:
  #    metadataUrl:
  #    entityId:
  #    cacerts:
  #  sp:
  #    entityId:
  #  exchangeKey: {}
  #  adminRole: {}
  #  viewerRole: {}
  #  developerRole: {}
  # -- Place here your settings, if you want to authenticate via *OIDC* method.
  # @default -- `(see example in values file)`
  oidc: {}
  #  enabled: false
  #  discoveryUrl: {}
  #  subjectKey: "email"
  #  rolesKey: "roles"
  #  adminRole: {}
  #  viewerRole: {}
  #  developerRole: {}
  #  cacerts: {}
  #  clientId: {}
  #  clientSecret: {}
  #  scope: "openid"
  #  verifyHostnames: true
  #  logoutUrl:

#Init container configuration. Used for multiple application startup checks
init_container:
  image: "nickytd/init-container"
  imageTag: "1.0.3"
  imagePullPolicy: IfNotPresent

# Configuration of OS cluster manager node
clusterManager:
  replicas: 1
  storage: "1Gi"
  heapSize: "256M"
  resources:
    requests:
      memory: "700Mi"
    limits:
      memory: "700Mi"
  podLabels: {}
  storageClass: {}
  priorityClass: {}
  tolerations: []
  affinity: {}

# Configuration of OS coordination node if "inCluster" is true
client:
  replicas: 1
  heapSize: "512M"
  resources:
    requests:
      memory: "1000Mi"
    limits:
      memory: "2000Mi"
  ingress:
    className: ""
    host: []
    path: "/"
    enabled: false
    annotations: {}
    tls: []
  podLabels: {}
  priorityClass: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: []
  #- maxSkew: 1
  #  topologyKey: kubernetes.io/hostname
  #  whenUnsatisfiable: ScheduleAnyway
  #  labelSelector:
  #    matchLabels:
  #      type: client

# Configuration of OS data node if "inCluster" is true
data:
  replicas: 1
  heapSize: "512M"
  storage: "1Gi"
  resources:
    requests:
      memory: "1000Mi"
    limits:
      memory: "2000Mi"
  storageClass: {}
  podLabels: {}
  priorityClass: {}
  tolerations: []
  affinity: {}

opensearch_dashboards:
  # dashboards custom branding configuration
  # https://opensearch.org/docs/latest/dashboards/branding/
  branding: {}
  image: "opensearchproject/opensearch-dashboards"
  imageTag: 2.4.0
  # When externalOpensearchDashboards.disabled designates an external
  # opensearch-dashboards instance. Only jobs for creating index
  # templates and opensearch-dashboards objects are executed.
  externalOpensearchDashboards:
    disabled: true
    runJob: false
    caCertificateSecret: ""
    url: ""
  replicas: 1
  extraEnvs: {}
  user: "opensearch"
  password: "opensearch"
  readonly:
    user: "viewer"
    password: "view"
  developer:
    user: "developer"
    password: "develop"
  ingress:
    className: ""
    host: []
    hosts: {}
    path: "/"
    enabled: false
    annotations: {}
    tls: []
  indexPatterns:
    - containers
    - systemd
    - nginx
  tenants:
    - Global
    - Developer
  resources:
    requests:
      memory: "500Mi"
    limits:
      memory: "500Mi"
  podLabels: {}
  priorityClass: {}
  tolerations: []
  affinity: {}

data_prepper:
  enabled: false
  image: opensearchproject/data-prepper
  imageTag: 2.0.1
  replicas: 1
  heapSize: "256M"
  resources:
    requests:
      memory: "600Mi"
    limits:
      memory: "600Mi"
  podLabels: {}
  priorityClass: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: {}
  #- maxSkew: 1
  #  topologyKey: kubernetes.io/hostname
  #  whenUnsatisfiable: ScheduleAnyway
  #  labelSelector:
  #    matchLabels:
  #      type: data-prepper

fluentbit:
  enabled: true
  image: "fluent/fluent-bit"
  imageTag: "2.0.8"
  imagePullPolicy: IfNotPresent
  containersLogsHostPath: /var/log/pods
  journalsLogsHostPath: /var/log
  #define container runtime: docker or containerd
  containersRuntime: containerd
  indexPrefix: ""
  disableTailInput: false
  mergeLog: On
  caCertificateSecret: ""
  readFromHead: false
  resources:
    requests:
      memory: "50Mi"
    limits:
      memory: "100Mi"
  podLabels: {}
  priorityClass: ""
  tolerations:
  - operator: Exists
  affinity: {}
  metrics:
    enabled: false
    interval: "30s"
    namespace: ""
  extraEnvs: {}

## Logstash is the recommended approach for delivering log stream to opensearch
## kafka -> logstash -> opensearch
## Note: kafka needs to be enabled as well
logstash:
  enabled: true
  image: "opensearchproject/logstash-oss-with-opensearch-output-plugin"
  imageTag: "8.4.0"
  replicas: 1
  heapSize: "256M"
  resources:
    requests:
      memory: "700Mi"
    limits:
      memory: "700Mi"
  podLabels: {}
  priorityClass: {}
  tolerations: []
  affinity: {}
  monitoring:
    enabled: false
    image: "nickytd/logstash-exporter"
    imageTag: "0.3.0"
    metricsPort: 9198
    serviceMonitor:
      enabled: false
      namespace: ""
  topologySpreadConstraints: []
  #- maxSkew: 1
  #  topologyKey: kubernetes.io/hostname
  #  whenUnsatisfiable: ScheduleAnyway
  #  labelSelector:
  #    matchLabels:
  #      type: logstash

# In scaled out setup kafka queues are used as ingestion points to accommodate
# spiked in the logging stream volumes.
kafka:
  enabled: true
  replicas: 1
  image: "bitnami/kafka"
  imageTag: "3.2.0"
  heapSize: "256M"
  storage: "10Gi"
  topics:
    - name: "containers"
      config: "max.message.bytes=10000000,retention.bytes=-1,retention.ms=3600000"
      # If not set, partitions default to the number of brokers
      # partitions: 1
  resources:
    requests:
      memory: "600Mi"
    limits:
      memory: "600Mi"
  storageClass: {}
  podLabels: {}
  priorityClass: {}
  tolerations: []
  affinity: {}

# Zookeeper is a dependency of kafka
zookeeper:
  replicas: 1
  image: "zookeeper"
  imageTag: "3.8.0"
  heapSize: "128M"
  storage: 1Gi
  resources:
    requests:
      memory: "300Mi"
    limits:
      memory: "300Mi"
  storageClass: {}
  podLabels: {}
  priorityClass: {}
  tolerations: []
  affinity: {}

# -- Additional annotations for jobs pods
additionalJobAnnotations: {}
  ### example ArgoCD requires its specific annotations
  #"argocd.argoproj.io/hook": Sync
  #"argocd.argoproj.io/sync-wave": "0"
  #"argocd.argoproj.io/hook-delete-policy": BeforeHookCreation,HookSucceeded

# -- Additional annotations for job pods
additionalJobPodAnnotations:
  #"sidecar.istio.io/inject": "false"

# -- Additional labels for job pods
additionalJobPodLabels: {}
